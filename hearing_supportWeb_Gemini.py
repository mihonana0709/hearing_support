# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import vosk
import json
import os
import av
import threading
import numpy as np
import time
import logging

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(
    filename='hearing_support.log',
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Voskãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
MODEL_PATH = "vosk-model-small-ja-0.22"

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "history" not in st.session_state:
    st.session_state.history = []
if "current_transcription" not in st.session_state:
    st.session_state.current_transcription = ""
if "volume_history" not in st.session_state:
    st.session_state.volume_history = []
if "vosk_model_loaded" not in st.session_state:
    st.session_state.vosk_model_loaded = False
if "recognizer" not in st.session_state:
    st.session_state.recognizer = None

# Voskãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿é–¢æ•°
@st.cache_resource
def load_vosk_model():
    """Voskãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦èª­ã¿è¾¼ã¿ã¾ã™ã€‚"""
    if not os.path.exists(MODEL_PATH) or not os.path.isdir(MODEL_PATH):
        st.error(f"âŒ Voskãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹: {os.path.abspath(MODEL_PATH)}")
        st.info("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆ`vosk-model-small-ja-0.22`ï¼‰ã‚’ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜å ´æ‰€ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
        logger.error(f"LOG: Voskãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {os.path.abspath(MODEL_PATH)}")
        st.session_state.vosk_model_loaded = False
        return None
    try:
        model = vosk.Model(MODEL_PATH)
        logger.info("LOG: Voskãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        st.session_state.vosk_model_loaded = True
        return model
    except Exception as e:
        st.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        logger.error(f"LOG: Voskãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.session_state.vosk_model_loaded = False
        return None

# AudioProcessorBaseã‚’ç¶™æ‰¿ã—ãŸã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹
class VoskAudioProcessor(AudioProcessorBase):
    def __init__(self):
        self.lock = threading.Lock()
        self.transcription_queue = []
        self.volume_history_queue = []
        
        # Voskèªè­˜ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
        vosk_model = st.session_state.get("vosk_model", None)
        if vosk_model:
            self.recognizer = vosk.KaldiRecognizer(vosk_model, 16000)
            logger.info("LOG: Voskèªè­˜ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚")
        else:
            self.recognizer = None
            logger.warning("LOG: Voskãƒ¢ãƒ‡ãƒ«ãŒæœªãƒ­ãƒ¼ãƒ‰ã®ãŸã‚ã€èªè­˜ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ã§ãã¾ã›ã‚“ã€‚")

    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        """WebRTCã‹ã‚‰å—ä¿¡ã—ãŸã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã¾ã™ã€‚"""
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Numpyé…åˆ—ã«å¤‰æ›
        audio_data = frame.to_ndarray(format="s16le")

        # éŸ³é‡ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—ã—ã€ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        rms = np.sqrt(np.mean(np.square(audio_data)))
        with self.lock:
            self.volume_history_queue.append(rms)

        # Voskã§éŸ³å£°èªè­˜ï¼ˆå‡¦ç†ã‚’è»½é‡åŒ–ï¼‰
        if self.recognizer and len(self.volume_history_queue) % 5 == 0:  # 5ãƒ•ãƒ¬ãƒ¼ãƒ ã«1å›ã ã‘å‡¦ç†
            if self.recognizer.AcceptWaveform(audio_data.tobytes()):
                result = self.recognizer.Result()
                text = json.loads(result)["text"]
                if text.strip():
                    with self.lock:
                        self.transcription_queue.append(text)
            else:
                partial_result = self.recognizer.PartialResult()
                partial_text = json.loads(partial_result)["partial"]
                if partial_text.strip():
                    with self.lock:
                        self.transcription_queue.append(f"partial:{partial_text}")
        
        return frame

# webrtc_streamerã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’é…ç½®
webrtc_ctx = webrtc_streamer(
    key="speech",
    mode=WebRtcMode.SENDRECV,
    audio_processor_factory=VoskAudioProcessor,
    media_stream_constraints={
        "audio": {
            "autoGainControl": True,
            "echoCancellation": True,
            "noiseSuppression": True,
            "channelCount": 1,
            "sampleRate": 16000
        },
        "video": False
    },
    rtc_configuration={"iceServers": [
        {"urls": ["stun:stun.l.google.com:19302"]},
        {"urls": ["stun:stun1.l.google.com:19302"]},
        {"urls": ["stun:stun2.l.google.com:19302"]},
    ]},
)

# ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®çŠ¶æ…‹è¡¨ç¤º
status_placeholder = st.empty()
if webrtc_ctx.state.playing:
    status_placeholder.info("ğŸ§ éŸ³å£°èªè­˜ä¸­...è©±ã—ã‹ã‘ã¦ãã ã•ã„")
else:
    status_placeholder.info("ğŸ›‘ åœæ­¢ä¸­")

# ---
### ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³é‡ãƒ¬ãƒ™ãƒ«

with st.sidebar:
    st.markdown("### ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³é‡ãƒ¬ãƒ™ãƒ«")

    if webrtc_ctx.state.playing and webrtc_ctx.audio_processor:
        with webrtc_ctx.audio_processor.lock:
            st.session_state.volume_history.extend(webrtc_ctx.audio_processor.volume_history_queue)
            webrtc_ctx.audio_processor.volume_history_queue.clear()
        
        if st.session_state.volume_history:
            st.line_chart(st.session_state.volume_history[-50:])
    else:
        st.info("ãƒã‚¤ã‚¯å…¥åŠ›ã‚’å¾…æ©Ÿä¸­ã§ã™...")

# ---
### ğŸ“ èªè­˜çµæœ

result_placeholder = st.empty()

if webrtc_ctx.state.playing and webrtc_ctx.audio_processor:
    with webrtc_ctx.audio_processor.lock:
        while webrtc_ctx.audio_processor.transcription_queue:
            text = webrtc_ctx.audio_processor.transcription_queue.pop(0)
            if text.startswith("partial:"):
                st.session_state.current_transcription = text.replace("partial:", "")
            else:
                st.session_state.history.append(text)
                st.session_state.current_transcription = ""

if st.session_state.current_transcription:
    result_placeholder.write(f"**ï¼ˆèªè­˜ä¸­ï¼‰** {st.session_state.current_transcription}")
elif st.session_state.history:
    result_placeholder.write(st.session_state.history[-1])
else:
    result_placeholder.info("ã“ã“ã«æ–‡å­—èµ·ã“ã—çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

# ---
### ğŸ“‹ å±¥æ­´# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import av
import numpy as np
import threading

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "volume_history" not in st.session_state:
    st.session_state.volume_history = []

# AudioProcessorBaseã‚’ç¶™æ‰¿ã—ãŸã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹
class SimpleAudioProcessor(AudioProcessorBase):
    def __init__(self):
        self.lock = threading.Lock()
        self.volume_history_queue = []
    
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        """WebRTCã‹ã‚‰å—ä¿¡ã—ãŸã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†ã—ã¾ã™ã€‚"""
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’Numpyé…åˆ—ã«å¤‰æ›
        audio_data = frame.to_ndarray(format="s16le")

        # éŸ³é‡ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—ã—ã€ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        rms = np.sqrt(np.mean(np.square(audio_data)))
        with self.lock:
            self.volume_history_queue.append(rms)

        # éŸ³é‡ãƒ–ãƒ¼ã‚¹ãƒˆå‡¦ç†ï¼ˆã‚²ã‚¤ãƒ³ã‚’èª¿æ•´å¯èƒ½ï¼‰
        gain = 2.0
        boosted_audio = np.clip(audio_data * gain, -32768, 32767).astype(np.int16)
        
        return frame.from_ndarray(boosted_audio)


# ---
### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³UI ###
st.title("ğŸ§ è´è¦šã‚µãƒãƒ¼ãƒˆã‚¢ãƒ—ãƒªï¼ˆãƒã‚¤ã‚¯ãƒ»ã‚¤ãƒ¤ãƒ›ãƒ³ãƒ†ã‚¹ãƒˆï¼‰")
st.write("ãƒã‚¤ã‚¯éŸ³å£°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å¢—å¹…ã—ã€ã‚¤ãƒ¤ãƒ›ãƒ³ã‹ã‚‰å†ç”Ÿã—ã¾ã™ã€‚")

# webrtc_streamerã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’é…ç½®
webrtc_ctx = webrtc_streamer(
    key="hearing_support",
    mode=WebRtcMode.SENDRECV,
    audio_processor_factory=SimpleAudioProcessor,
    media_stream_constraints={
        "audio": {
            "autoGainControl": True,
            "echoCancellation": True,
            "noiseSuppression": True,
        },
        "video": False
    },
)

# ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®çŠ¶æ…‹è¡¨ç¤º
status_placeholder = st.empty()
if webrtc_ctx.state.playing:
    status_placeholder.info("ğŸ§ ãƒã‚¤ã‚¯ã¨ã‚¤ãƒ¤ãƒ›ãƒ³ã®æ¥ç¶šãƒ†ã‚¹ãƒˆä¸­...")
else:
    status_placeholder.info("ğŸ›‘ åœæ­¢ä¸­")

# ---
### ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³é‡ãƒ¬ãƒ™ãƒ«

with st.sidebar:
    st.markdown("### ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³é‡ãƒ¬ãƒ™ãƒ«")

    if webrtc_ctx.state.playing and webrtc_ctx.audio_processor:
        with webrtc_ctx.audio_processor.lock:
            st.session_state.volume_history.extend(webrtc_ctx.audio_processor.volume_history_queue)
            webrtc_ctx.audio_processor.volume_history_queue.clear()
        
        if st.session_state.volume_history:
            st.line_chart(st.session_state.volume_history[-50:])
    else:
        st.info("ãƒã‚¤ã‚¯å…¥åŠ›ã‚’å¾…æ©Ÿä¸­ã§ã™...")

if st.session_state.history:
    for line in reversed(st.session_state.history):
        st.write(f"- {line}")