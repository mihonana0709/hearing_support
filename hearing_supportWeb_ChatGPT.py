# å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:
# pip install streamlit streamlit-webrtc av vosk numpy

import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import av
import numpy as np
import vosk
import json
import time
import logging
import os

# -------------------------
# è¨­å®šãƒ»åˆæœŸåŒ–
# -------------------------
st.set_page_config(page_title="è´è¦šã‚µãƒãƒ¼ãƒˆã‚¢ãƒ—ãƒª", layout="wide")

logging.basicConfig(
    filename=os.path.join(os.getcwd(), "hearing_support.log"),
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)

MODEL_PATH = "vosk-model-small-ja-0.22"  # äº‹å‰ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’é…ç½®

if "history" not in st.session_state:
    st.session_state.history = []
if "volume_history" not in st.session_state:
    st.session_state.volume_history = []
if "partial_text" not in st.session_state:
    st.session_state.partial_text = ""
if "is_recognizing" not in st.session_state:
    st.session_state.is_recognizing = False  # é–‹å§‹/åœæ­¢ãƒœã‚¿ãƒ³ç”¨

# -------------------------
# Voskãƒ¢ãƒ‡ãƒ«ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
# -------------------------
@st.cache_resource(show_spinner=True)
def load_vosk_model():
    if not os.path.isdir(MODEL_PATH):
        raise FileNotFoundError(
            f"Voskãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MODEL_PATH}\n"
            "https://alphacephei.com/vosk/models ã‹ã‚‰ ja ã®ãƒ¢ãƒ‡ãƒ«ã‚’è§£å‡ã—ã¦é…ç½®ã—ã¦ãã ã•ã„ã€‚"
        )
    model = vosk.Model(MODEL_PATH)
    return model

# -------------------------
# ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ—ãƒ­ã‚»ãƒƒã‚µ
# -------------------------
class AudioProcessor(AudioProcessorBase):
    def __init__(self) -> None:
        self.model = load_vosk_model()
        self.recognizer = vosk.KaldiRecognizer(self.model, 16000)
        self.recognizer.SetWords(True)

        # å‡ºåŠ›å´ã®éŸ³é‡ï¼ˆGUIã‹ã‚‰æ›¸ãæ›ãˆã‚‹ï¼‰
        self.gain = 1.0

        # ãƒ¬ãƒ™ãƒ«è¡¨ç¤ºç”¨
        self.level_rms = 0.0

        # èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ
        self.partial_text = ""
        self.final_texts = []

        # å¤‰æ›ï¼ˆãƒªã‚µãƒ³ãƒ—ãƒ«ï¼‰ç”¨
        self.resampler_16k_mono = av.audio.resampler.AudioResampler(
            format="s16", layout="mono", rate=16000
        )

    def _to_int16(self, x: np.ndarray) -> np.ndarray:
        # æœŸå¾…ã™ã‚‹dtypeã«æƒãˆã‚‹
        if x.dtype != np.int16:
            x = np.clip(x, -1.0, 1.0)
            x = (x * 32767.0).astype(np.int16)
        return x

    def recv_audio(self, frame: av.AudioFrame) -> av.AudioFrame:
        # ndarray: shape=(channels, samples)
        samples = frame.to_ndarray()
        # ãƒ¬ãƒ™ãƒ«è¨ˆç®—ï¼ˆfloatåŒ–ã—ã¦RMSï¼‰
        f32 = samples.astype(np.float32)
        # æ­£è¦åŒ–ï¼ˆint16æƒ³å®šï¼‰
        if samples.dtype == np.int16:
            f32 = f32 / 32768.0
        rms = np.sqrt(np.mean(f32**2))
        self.level_rms = float(rms)

        # ---- éŸ³é‡èª¿æ•´ï¼ˆå‡ºåŠ›ç”¨ï¼‰ ----
        # å‡ºåŠ›ã¯å…¥åŠ›ã¨åŒã˜ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§è¿”ã™ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶å‡ºåŠ›ç”¨ï¼‰
        out = f32 * float(self.gain)
        out = np.clip(out, -1.0, 1.0)
        out_int16 = (out * 32767.0).astype(np.int16)

        # ---- éŸ³å£°èªè­˜ï¼ˆ16kHz, monoï¼‰----
        try:
            # frame -> 16k mono
            mono16k_frame = self.resampler_16k_mono.resample(frame)
            mono16k = mono16k_frame.to_ndarray()  # shape=(1, n)
            mono16k = self._to_int16(mono16k)
            pcm_bytes = mono16k.tobytes()

            if st.session_state.get("is_recognizing", False):
                if self.recognizer.AcceptWaveform(pcm_bytes):
                    result = json.loads(self.recognizer.Result())
                    text = (result.get("text") or "").strip()
                    if text:
                        self.final_texts.append(text)
                        self.partial_text = ""
                else:
                    presult = json.loads(self.recognizer.PartialResult())
                    ptext = (presult.get("partial") or "").strip()
                    self.partial_text = ptext
            else:
                # åœæ­¢ä¸­ã¯éƒ¨åˆ†çµæœã‚’ã‚¯ãƒªã‚¢
                self.partial_text = ""
        except Exception as e:
            logger.error(f"recognition error: {e}")

        # å¤‰æ›´ã—ãŸã‚µãƒ³ãƒ—ãƒ«ã§AudioFrameã‚’ä½œã£ã¦è¿”ã™ï¼ˆã‚¤ãƒ¤ãƒ›ãƒ³ã¸å‡ºåŠ›ï¼‰
        out_frame = av.AudioFrame.from_ndarray(out_int16, layout=frame.layout)
        out_frame.sample_rate = frame.sample_rate
        return out_frame

# -------------------------
# UI
# -------------------------
st.title("ğŸ¤ è´è¦šã‚µãƒãƒ¼ãƒˆã‚¢ãƒ—ãƒª")
st.caption("ãƒã‚¤ã‚¯å…¥åŠ›ã‚’**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ–‡å­—èµ·ã“ã—**ã—ã€**éŸ³é‡ã‚’èª¿æ•´**ã—ã¦ã‚¤ãƒ¤ãƒ›ãƒ³ã¸è¿”ã—ã¾ã™ã€‚")

col_left, col_right = st.columns([2, 1])

with col_right:
    st.subheader("âš™ï¸ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«")
    volume = st.slider("ğŸ”Š éŸ³é‡ï¼ˆå‡ºåŠ›ã‚²ã‚¤ãƒ³ï¼‰", 0.0, 3.0, 1.0, 0.05, key="gain_slider")

    c1, c2 = st.columns(2)
    if c1.button("â–¶ï¸ èªè­˜é–‹å§‹", use_container_width=True):
        st.session_state.is_recognizing = True
    if c2.button("â¹ èªè­˜åœæ­¢", use_container_width=True):
        st.session_state.is_recognizing = False

    if st.button("ğŸ§¹ å±¥æ­´ã‚¯ãƒªã‚¢", use_container_width=True):
        st.session_state.history.clear()

    st.markdown("---")
    st.markdown("### ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³é‡ãƒ¬ãƒ™ãƒ«")
    # å¾Œã§éšæ™‚æ›´æ–°
    chart_placeholder = st.empty()

with col_left:
    st.subheader("ğŸ“ èªè­˜çµæœ")
    partial_placeholder = st.empty()
    history_placeholder = st.empty()

st.markdown("---")
st.caption(
    "â€» PCã®å†ç”ŸéŸ³ã‚’æ–‡å­—èµ·ã“ã—ã—ãŸã„å ´åˆã¯ã€Œã‚¹ãƒ†ãƒ¬ã‚ªãƒŸã‚­ã‚µãƒ¼ã€ã‚„ä»®æƒ³ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒ‡ãƒã‚¤ã‚¹ï¼ˆVB-CABLEç­‰ï¼‰ã‚’ãƒã‚¤ã‚¯å…¥åŠ›ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚"
)

# -------------------------
# WebRTC
# -------------------------
webrtc_ctx = webrtc_streamer(
    key="speech",
    mode=WebRtcMode.SENDRECV,   # å—ä¿¡ã—ãŸéŸ³å£°ã‚’å‡¦ç†ã—ã¦é€ã‚Šè¿”ã™
    audio_receiver_size=1024,
    media_stream_constraints={
        "audio": {
            "autoGainControl": True,
            "echoCancellation": True,
            "noiseSuppression": True,
        },
        "video": False,
    },
    async_processing=True,  # éŸ³å£°å‡¦ç†ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§
)

# -------------------------
# çŠ¶æ…‹ã®åæ˜ ï¼†è¡¨ç¤ºæ›´æ–°
# -------------------------
if webrtc_ctx and webrtc_ctx.state.playing:
    # AudioProcessorã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
    ap: AudioProcessor = webrtc_ctx.audio_processor
    if ap is not None:
        # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼å€¤ã‚’ãƒ—ãƒ­ã‚»ãƒƒã‚µã«ä¼ãˆã‚‹ï¼ˆå‡ºåŠ›ãƒœãƒªãƒ¥ãƒ¼ãƒ ï¼‰
        ap.gain = float(volume)

        # ãƒ¬ãƒ™ãƒ«å±¥æ­´ã‚’æ›´æ–°ï¼ˆæœ€å¤§200ç‚¹ï¼‰
        st.session_state.volume_history.append(ap.level_rms)
        if len(st.session_state.volume_history) > 200:
            st.session_state.volume_history = st.session_state.volume_history[-200:]

        # éƒ¨åˆ†çµæœ
        st.session_state.partial_text = ap.partial_text

        # ç¢ºå®šçµæœã‚’å±¥æ­´ã«å–ã‚Šè¾¼ã¿
        if ap.final_texts:
            st.session_state.history.extend(ap.final_texts)
            ap.final_texts = []

    # ç”»é¢è¡¨ç¤ºã‚’æ›´æ–°
    if st.session_state.volume_history:
        chart_placeholder.line_chart(st.session_state.volume_history)
    else:
        chart_placeholder.info("å…¥åŠ›å¾…æ©Ÿä¸­â€¦")

    if st.session_state.partial_text:
        partial_placeholder.markdown(f"**ã„ã¾èã“ãˆãŸ:** {st.session_state.partial_text}")
    else:
        partial_placeholder.markdown("_ï¼ˆéƒ¨åˆ†çµæœãªã—ï¼‰_")

    if st.session_state.history:
        # æ–°ã—ã„é †ã§æ•°ä»¶ã ã‘
        latest = st.session_state.history[-40:]
        history_md = "\n".join(f"- {line}" for line in latest[::-1])
        history_placeholder.markdown(history_md)
    else:
        history_placeholder.info("ã“ã“ã«ç¢ºå®šã—ãŸæ–‡å­—èµ·ã“ã—ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
else:
    chart_placeholder = chart_placeholder if "chart_placeholder" in locals() else st.empty()
    chart_placeholder.info("æ¥ç¶šã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
