# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import av
import numpy as np
import threading
import queue
import vosk
import json
import os
import logging
from scipy.signal import butter, lfilter

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(
    filename='hearing_support.log',
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Voskãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
MODEL_PATH = "vosk-model-small-ja-0.22"

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "history" not in st.session_state:
    st.session_state.history = []
if "current_transcription" not in st.session_state:
    st.session_state.current_transcription = ""
if "volume_history" not in st.session_state:
    st.session_state.volume_history = []
if "processing_thread" not in st.session_state:
    st.session_state.processing_thread = None

# ç¢ºå®Ÿã«ã‚­ãƒ¥ãƒ¼ã‚’åˆæœŸåŒ–ã™ã‚‹
if "audio_queue" not in st.session_state:
    st.session_state.audio_queue = queue.Queue()
if "transcription_queue" not in st.session_state:
    st.session_state.transcription_queue = queue.Queue()
if "volume_queue" not in st.session_state:
    st.session_state.volume_queue = queue.Queue()


# Voskãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿é–¢æ•° (ã‚­ãƒ£ãƒƒã‚·ãƒ¥)
@st.cache_resource
def load_vosk_model():
    """Voskãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦èª­ã¿è¾¼ã¿ã¾ã™ã€‚"""
    if not os.path.exists(MODEL_PATH) or not os.path.isdir(MODEL_PATH):
        st.error(f"âŒ Voskãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹: {os.path.abspath(MODEL_PATH)}")
        st.info("ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆ`vosk-model-small-ja-0.22`ï¼‰ã‚’ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜å ´æ‰€ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
        return None
    try:
        model = vosk.Model(MODEL_PATH)
        logger.info("LOG: Voskãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return model
    except Exception as e:
        st.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        logger.error(f"LOG: Voskãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

# ãƒã‚¿ãƒ¼ãƒ¯ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è¨­è¨ˆ
def butter_bandpass(lowcut, highcut, fs, order=5):
    nyquist = 0.5 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    return b, a

# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å‡¦ç†
def bandpass_filter(data, lowcut, highcut, fs, order=5):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    y = lfilter(b, a, data)
    return y

# Voskã®éŸ³å£°èªè­˜å‡¦ç†ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§è¡Œã†ã‚¯ãƒ©ã‚¹
class AudioProcessingThread(threading.Thread):
    def __init__(self, audio_queue, transcription_queue, volume_queue, gain_factor, low_freq_boost, high_freq_boost):
        super().__init__()
        self.audio_queue = audio_queue
        self.transcription_queue = transcription_queue
        self.volume_queue = volume_queue
        self.gain_factor = gain_factor
        self.low_freq_boost = low_freq_boost
        self.high_freq_boost = high_freq_boost
        self.stop_event = threading.Event()
        self.recognizer = vosk.KaldiRecognizer(load_vosk_model(), 16000)
        logger.info("LOG: AudioProcessingThread åˆæœŸåŒ–å®Œäº†ã€‚")

    def run(self):
        logger.info("LOG: AudioProcessingThread é–‹å§‹ã€‚")
        while not self.stop_event.is_set():
            try:
                frame = self.audio_queue.get(timeout=1)
                
                audio_data = frame.to_ndarray()
                
                # ã‚¤ã‚³ãƒ©ã‚¤ã‚¶ãƒ¼å‡¦ç†
                if self.low_freq_boost != 0:
                    low_freq_data = bandpass_filter(audio_data.copy(), 20, 500, 16000)
                    audio_data += low_freq_data * self.low_freq_boost
                if self.high_freq_boost != 0:
                    high_freq_data = bandpass_filter(audio_data.copy(), 2000, 8000, 16000)
                    audio_data += high_freq_data * self.high_freq_boost
                
                # å…¨ä½“ã‚²ã‚¤ãƒ³ã‚’é©ç”¨
                amplified_data = audio_data * self.gain_factor
                audio_data_int16 = amplified_data.astype(np.int16)
                
                if audio_data_int16.size > 0:
                    audio_data_float = audio_data_int16.astype(np.float64)
                    rms = np.sqrt(np.mean(np.square(audio_data_float)))
                    self.volume_queue.put(rms)
                    logger.info(f"LOG: éŸ³å£°ãƒ‡ãƒ¼ã‚¿å–å¾—ã€RMS={rms:.2f}")

                if self.recognizer.AcceptWaveform(audio_data_int16.tobytes()):
                    result = self.recognizer.Result()
                    text = json.loads(result)["text"]
                    if text.strip():
                        self.transcription_queue.put(text)
                        logger.info(f"LOG: èªè­˜çµæœ -> {text}")
                else:
                    partial_result = self.recognizer.PartialResult()
                    partial_text = json.loads(partial_result)["partial"]
                    if partial_text.strip():
                        self.transcription_queue.put(f"partial:{partial_text}")

            except queue.Empty:
                continue

    def stop(self):
        self.stop_event.set()
        logger.info("LOG: AudioProcessingThread åœæ­¢ä¿¡å·å—ä¿¡ã€‚")

# AudioProcessorBaseã‚’ç¶™æ‰¿ã—ãŸWebRTCç”¨ã®ã‚¯ãƒ©ã‚¹
class WebRtcAudioProcessor(AudioProcessorBase):
    def __init__(self, audio_queue):
        self.audio_queue = audio_queue
        logger.info("LOG: WebRtcAudioProcessor åˆæœŸåŒ–å®Œäº†ã€‚")

    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        self.audio_queue.put(frame)
        logger.info("LOG: WebRTCãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚­ãƒ¥ãƒ¼ã«æ ¼ç´ã€‚")
        return frame

# ---
### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³UI ###
st.title("ğŸ¤ è´è¦šã‚µãƒãƒ¼ãƒˆã‚¢ãƒ—ãƒª")
st.write("ãƒã‚¤ã‚¯éŸ³å£°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å¢—å¹…ãƒ»æ–‡å­—èµ·ã“ã—ã—ã€ã‚¤ãƒ¤ãƒ›ãƒ³ã‹ã‚‰å†ç”Ÿã—ã¾ã™ã€‚")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«éŸ³é‡ãƒ»éŸ³è³ªèª¿æ•´ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ 
with st.sidebar:
    st.markdown("### ğŸ”Š éŸ³å£°èª¿æ•´")
    gain_slider = st.slider(
        "å…¨ä½“éŸ³é‡ï¼ˆã‚²ã‚¤ãƒ³ï¼‰",
        min_value=1.0,
        max_value=10.0,
        value=3.0,
        step=0.5,
        help="ãƒã‚¤ã‚¯ã§æ‹¾ã£ãŸéŸ³ã‚’å¢—å¹…ã™ã‚‹ãƒ¬ãƒ™ãƒ«ã‚’èª¿æ•´ã—ã¾ã™ã€‚"
    )

    low_freq_boost_slider = st.slider(
        "ä½éŸ³åŸŸèª¿æ•´",
        min_value=-2.0,
        max_value=2.0,
        value=0.0,
        step=0.1,
        help="ä½éŸ³åŸŸï¼ˆã€œ500Hzï¼‰ã®éŸ³é‡ã‚’èª¿æ•´ã—ã¾ã™ã€‚ãƒã‚¤ã‚ºãŒæ°—ã«ãªã‚‹å ´åˆã¯ä¸‹ã’ã¦ãã ã•ã„ã€‚"
    )

    high_freq_boost_slider = st.slider(
        "é«˜éŸ³åŸŸèª¿æ•´",
        min_value=-2.0,
        max_value=2.0,
        value=0.0,
        step=0.1,
        help="é«˜éŸ³åŸŸï¼ˆ2kHzã€œ8kHzï¼‰ã®éŸ³é‡ã‚’èª¿æ•´ã—ã¾ã™ã€‚è´åŠ›ã«åˆã‚ã›ã¦ä¸Šã’ã‚‹ã¨èãå–ã‚Šã‚„ã™ããªã‚Šã¾ã™ã€‚"
    )

    st.markdown("---")
    st.markdown("### ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³é‡ãƒ¬ãƒ™ãƒ«")

    while not st.session_state.volume_queue.empty():
        st.session_state.volume_history.append(st.session_state.volume_queue.get())
        if len(st.session_state.volume_history) > 100:
            st.session_state.volume_history.pop(0)

    if "webrtc_ctx" in st.session_state and st.session_state.webrtc_ctx.state.playing:
        st.line_chart(st.session_state.volume_history)
    else:
        st.info("ãƒã‚¤ã‚¯å…¥åŠ›ã‚’å¾…æ©Ÿä¸­ã§ã™...")


# webrtc_ctxã®åˆæœŸåŒ–ã‚’ try-except ã§å›²ã‚€
try:
    processor = WebRtcAudioProcessor(st.session_state.audio_queue)
    webrtc_ctx = webrtc_streamer(
        key="speech",
        mode=WebRtcMode.SENDRECV,
        audio_processor_factory=lambda: processor,
        media_stream_constraints={
            "audio": True,
            "video": False
        },
    )
    st.session_state.webrtc_ctx = webrtc_ctx

except Exception as e:
    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    st.stop()


# ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®çŠ¶æ…‹è¡¨ç¤º
status_placeholder = st.empty()
if webrtc_ctx.state.playing:
    status_placeholder.info("ğŸ§ éŸ³å£°èªè­˜ä¸­...è©±ã—ã‹ã‘ã¦ãã ã•ã„")
    if st.session_state.processing_thread is None or not st.session_state.processing_thread.is_alive():
        st.session_state.processing_thread = AudioProcessingThread(
            st.session_state.audio_queue, 
            st.session_state.transcription_queue, 
            st.session_state.volume_queue,
            gain_slider,
            low_freq_boost_slider,
            high_freq_boost_slider
        )
        st.session_state.processing_thread.start()
elif not webrtc_ctx.state.playing and st.session_state.processing_thread is not None:
    st.session_state.processing_thread.stop()
    st.session_state.processing_thread.join()
    st.session_state.processing_thread = None
    status_placeholder.info("ğŸ›‘ åœæ­¢ä¸­")
else:
    status_placeholder.info("ğŸ›‘ åœæ­¢ä¸­")


# ---
### ğŸ“ èªè­˜çµæœ

result_placeholder = st.empty()

while not st.session_state.transcription_queue.empty():
    text = st.session_state.transcription_queue.get()
    if text.startswith("partial:"):
        st.session_state.current_transcription = text.replace("partial:", "")
    else:
        st.session_state.history.append(text)
        st.session_state.current_transcription = ""

if st.session_state.current_transcription:
    result_placeholder.write(f"**ï¼ˆèªè­˜ä¸­ï¼‰** {st.session_state.current_transcription}")
elif st.session_state.history:
    result_placeholder.write(st.session_state.history[-1])
else:
    result_placeholder.info("ã“ã“ã«æ–‡å­—èµ·ã“ã—çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

# ---
### ğŸ“‹ å±¥æ­´

if st.session_state.history:
    st.markdown("### ğŸ“‹ å±¥æ­´")
    for line in reversed(st.session_state.history):
        st.write(f"- {line}")